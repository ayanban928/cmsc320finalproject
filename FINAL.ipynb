{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d30add9-c411-4cf1-b160-6dd464aef26b",
   "metadata": {},
   "source": [
    "# TITLE HERE\n",
    "\n",
    "Spring 2025 CMSC320 Final Project\n",
    "\n",
    "Collaborators: Marvin Lin, Christopher Su, Tanish Bollam, Ayan Banerjee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61641052-2156-4f89-a649-d513b4c2f531",
   "metadata": {},
   "source": [
    "### Contributions:\n",
    "\n",
    "Marvin:\n",
    "\n",
    "Chris:\n",
    "\n",
    "Tanish:\n",
    "\n",
    "Ayan:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309aa67e-afcd-4688-922a-c1c5b7f336a4",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1d585-8acb-42ae-8999-1a476ab020d3",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "For our final project in CMSC320, we are analyzing the Rossmann Store Sales dataset, originally published by Florian Knauer and Will Cukierski (2015) on Kaggle. This dataset contains historical sales data for over 1,000 Rossmann stores across several years, with features including daily sales figures, customer counts, promotional activity, store-specific attributes, and more.\n",
    "\n",
    "**Research Question**\n",
    "Our primary goal is to answer the question:\n",
    "Can we accurately predict daily sales for a given store using historical data and store-level features?\n",
    "To do this, we will develop a machine learning model that forecasts future store sales based on past trends and available metadata.\n",
    "\n",
    "**Motivation**\n",
    "Forecasting sales is a crucial task for retail businesses. Accurate predictions enable better inventory planning, workforce allocation, and promotional strategy. With the rise of data-driven decision-making, being able to reliably anticipate future demand gives stores a competitive edge. By tackling this problem, we hope to gain insight into how various factors—such as promotions, holidays, and store type—impact daily sales performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d2c9c-98fc-4300-8a05-8e830b833976",
   "metadata": {},
   "source": [
    "## Data Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf5441-0719-416e-b5a1-f03cfd84beea",
   "metadata": {},
   "source": [
    "This part of the project involves searching for and collecting relevant data from several sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979e3ec-3ebe-4112-90a2-4337110a7873",
   "metadata": {},
   "source": [
    "We will begin by importing relevant Python libraries necessary for this process.\n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f7c72d-d729-42d1-9e06-2c96409f4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for hypothesis testing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171a4eb-87dd-4628-9c3d-493beb482b9c",
   "metadata": {},
   "source": [
    "These libraries are necessary in the data science process. \n",
    "\n",
    "* **Pandas** is a library used for data manipulation and analysis. It provides data structures such as DataFrame and Series that allow us to efficiently handle structured data.\n",
    "* **Numpy** is a a fundamental library for numerical computing. It offers support for arrays, functions, linear algebra, and statistical operations among functionality.\n",
    "* **Scikit-learn** is a library primarily used for machine learning. It provides efficent tools for data analysis and modeling. These tasks include classification, regression, clustering, and dimensionality reduction.\n",
    "* **Scipy-stats** is a module within scipy the SciPy library that provides various functions for statistical computations. These include probability distributions, statistical tests, descriptive statistics, etc.\n",
    "* **Matplotlib** and **seaborn** are data visualization libraries for creating visualizations in Python such as line graphs, histograms, bar charts, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916c50b-53d8-4d83-a82f-d20a629c78a9",
   "metadata": {},
   "source": [
    "Next, we must choose a relevant dataset for our topic. Based on our objective of predicting future store sales based on historical data, we chose the [Rossmann Store Sales dataset](https://www.kaggle.com/competitions/rossmann-store-sales) to work with. Rossmann is one of the largest drug store chains in Europe, with over 4000 stores. This dataset includes key features such as Date, Customers, Sales, Store information, Promotions, etc that will be important for predicting future sales. It includes historical sales data for 1,115 Rossmann stores. \n",
    "\n",
    "The data is split into the following files:\n",
    "* **train.csv** - historical data including Sales.\n",
    "* **test.csv** - historical data excluding Sales.\n",
    "* **store.csv** - supplemental information about the stores.\n",
    "\n",
    "After downloading these datasets, we can begin by loading them into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "954b6c2a-bd58-43d5-b2a1-26f401b6ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into pandas DataFrames\n",
    "train_df = pd.read_csv('DATASETS/train.csv', low_memory=False)\n",
    "test_df = pd.read_csv('DATASETS/test.csv', low_memory=False)\n",
    "store_df = pd.read_csv('DATASETS/store.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb108ed3-1f3a-43ba-86af-fd221fe035d8",
   "metadata": {},
   "source": [
    "We use these two datasets and exclude **test.csv** because the information is already included in **train.csv**. The next step is to combine these into one DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c170a-a613-4c12-a525-db0b86e900f1",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10160a1e-ce09-445e-ba72-2fd447195d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(train_df, store_df, on='Store', how='left')\n",
    "test_df = pd.merge(test_df, store_df, on='Store', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ceb1c2",
   "metadata": {},
   "source": [
    "The **train_df** and **test_df** dataframes only include a Store identifier. However, each store has associated attributes (StoreType, Assortment, CompetitionDistance, etc.) in store_df that are crucial for modeling sales. The how='left' parameter in pd.merge ensures that all rows from train_df are preserved. \n",
    "\n",
    "By doing a left join on the dataframes, we ensure every record in your training and testing data includes relevant store features. This helps your machine learning model learn how differences between stores affect sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9241d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode all the following columns\n",
    "cols = ['StateHoliday', 'StoreType', 'Assortment', 'PromoInterval']\n",
    "for col in cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2af21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting string numbers into just numbers\n",
    "cols = ['Open', 'Promo', 'Promo2', 'SchoolHoliday']\n",
    "for col in cols:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: text/plain>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Store                          int64\n",
       "DayOfWeek                      int64\n",
       "Date                          object\n",
       "Sales                          int64\n",
       "Customers                      int64\n",
       "Open                           int64\n",
       "Promo                          int64\n",
       "StateHoliday                  object\n",
       "SchoolHoliday                  int64\n",
       "StoreType                     object\n",
       "Assortment                    object\n",
       "CompetitionDistance          float64\n",
       "CompetitionOpenSinceMonth    float64\n",
       "CompetitionOpenSinceYear     float64\n",
       "Promo2                         int64\n",
       "Promo2SinceWeek              float64\n",
       "Promo2SinceYear              float64\n",
       "PromoInterval                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Store                          int64\n",
       "DayOfWeek                      int64\n",
       "Date                          object\n",
       "Sales                          int64\n",
       "Customers                      int64\n",
       "Open                           int32\n",
       "Promo                          int32\n",
       "StateHoliday                   int32\n",
       "SchoolHoliday                  int32\n",
       "StoreType                      int32\n",
       "Assortment                     int32\n",
       "CompetitionDistance          float64\n",
       "CompetitionOpenSinceMonth    float64\n",
       "CompetitionOpenSinceYear     float64\n",
       "Promo2                         int32\n",
       "Promo2SinceWeek              float64\n",
       "Promo2SinceYear              float64\n",
       "PromoInterval                  int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <modified: text/plain>\n"
     ]
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472d58d-995a-4468-a28a-f9f130920533",
   "metadata": {},
   "source": [
    "## Data Exploration & Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79667e22-4648-45cb-aeb0-887c473e0290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82731370-a76d-4735-abc2-aae6fff99f8f",
   "metadata": {},
   "source": [
    "## ML Algorithm Design & Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc940a-4642-4cf3-8d66-dd70bb3a13af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Linear Regression ---\n",
      "CV RMSE (mean ± std): 2800.26 ± 76.48\n",
      "\n",
      "--- Random Forest ---\n",
      "CV RMSE (mean ± std): 1132.34 ± 97.21\n",
      "\n",
      "--- XGBoost ---\n",
      "CV RMSE (mean ± std): 1818.25 ± 32.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Filter dataset\n",
    "df_model = df[(df[\"Open\"] == 1) & (df[\"Sales\"] > 0)].copy()\n",
    "\n",
    "# Extract date\n",
    "df_model[\"Date\"] = pd.to_datetime(df_model[\"Date\"])\n",
    "df_model[\"Year\"] = df_model[\"Date\"].dt.year\n",
    "df_model[\"Month\"] = df_model[\"Date\"].dt.month\n",
    "df_model[\"Day\"] = df_model[\"Date\"].dt.day\n",
    "\n",
    "# label encoding\n",
    "label_cols = ['StateHoliday', 'StoreType', 'Assortment', 'PromoInterval']\n",
    "for col in label_cols:\n",
    "    df_model[col] = df_model[col].fillna(\"Missing\").astype(str)\n",
    "    df_model[col] = LabelEncoder().fit_transform(df_model[col])\n",
    "\n",
    "# remove unused columns\n",
    "df_model.drop([\"Date\", \"Customers\", \"Open\"], axis=1, inplace=True)\n",
    "\n",
    "# split features and target\n",
    "X = df_model.drop(\"Sales\", axis=1)\n",
    "y = df_model[\"Sales\"]\n",
    "\n",
    "# handle missing values\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
    "}\n",
    "\n",
    "# score\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "# cross validate with k-fold (5 folds)\n",
    "for name, model in models.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring=rmse_scorer)\n",
    "    print(f\"CV RMSE (mean ± std): {-scores.mean():.2f} ± {scores.std():.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379eb93",
   "metadata": {},
   "source": [
    "In our 5-fold cross-validation on the Rossmann sales data, Random Forest achieved the best performance with a mean RMSE of 1132.34, significantly outperforming both XGBoost (1818.25) and Linear Regression (2800.26). This indicates that Random Forest is well-suited for capturing the dataset’s non-linear patterns. XGBoost, while more consistent across folds, underperformed likely due to untuned hyperparameters. Linear Regression had the highest error, suggesting it is too simplistic for the complexity of the sales dynamics. Overall, Random Forest currently offers the most accurate and balanced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a1c35b-f88e-4b5c-a323-eb78c4a169af",
   "metadata": {},
   "source": [
    "## ML Algorithm Training & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421ba90-b730-4322-bc1c-3f39ad46e0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abb58869-5e08-44e3-8d5e-e4762c1b97fa",
   "metadata": {},
   "source": [
    "## Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670aef31-57d1-45bf-94de-57155151d000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
